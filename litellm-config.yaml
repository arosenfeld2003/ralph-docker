# LiteLLM Configuration for Ralph Docker
# Translates API calls to Ollama format
# Docs: https://docs.litellm.ai/docs/providers/ollama
#
# NOTE: host.docker.internal resolves to the host machine.
# This works on Docker Desktop (macOS/Windows) and Linux Docker 20.10+.
# For older Linux Docker, set DOCKER_HOST_IP in your .env file.
#
# Only list models that are actually pulled in Ollama.
# Check with: ollama list

model_list:
  - model_name: ollama/qwen2.5-coder:7b
    litellm_params:
      model: ollama/qwen2.5-coder:7b
      api_base: http://host.docker.internal:11434
      drop_params: true
    model_info:
      supports_thinking: false
      supports_extended_thinking: false

  - model_name: ollama/devstral
    litellm_params:
      model: ollama/devstral
      api_base: http://host.docker.internal:11434
      drop_params: true
    model_info:
      supports_thinking: false
      supports_extended_thinking: false

  - model_name: ollama/devstral:latest
    litellm_params:
      model: ollama/devstral:latest
      api_base: http://host.docker.internal:11434
      drop_params: true
    model_info:
      supports_thinking: false
      supports_extended_thinking: false

litellm_settings:
  # Drop unsupported params instead of erroring
  drop_params: true
  # Log for debugging
  set_verbose: true
  # Force disable thinking for all models
  disable_thinking: true

general_settings:
  # Master key for local use (required for API access)
  master_key: sk-ralph-local
