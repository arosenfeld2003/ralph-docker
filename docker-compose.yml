services:
  # Ralph - containerized autonomous development loop
  # Auth: ANTHROPIC_API_KEY env var, or `docker compose run --rm ralph login`
  ralph:
    build: .
    volumes:
      # Mount the workspace (project) directory
      - ${WORKSPACE_PATH:-.}:/home/ralph/workspace
      # Mount Claude config directory (needs write access for debug/todos and login credentials)
      - ${CLAUDE_CONFIG:-~/.claude}:/home/ralph/.claude
      # Mount SSH keys for git push (read-only)
      - ${SSH_DIR:-~/.ssh}:/home/ralph/.ssh:ro
    environment:
      - ANTHROPIC_API_KEY=${ANTHROPIC_API_KEY:-}
      - RALPH_MODE=${RALPH_MODE:-build}
      - RALPH_MAX_ITERATIONS=${RALPH_MAX_ITERATIONS:-0}
      - RALPH_MODEL=${RALPH_MODEL:-opus}
      - RALPH_OUTPUT_FORMAT=${RALPH_OUTPUT_FORMAT:-pretty}
      - RALPH_PUSH_AFTER_COMMIT=${RALPH_PUSH_AFTER_COMMIT:-true}
      - RALPH_ENTIRE_ENABLED=${RALPH_ENTIRE_ENABLED:-false}
      - RALPH_ENTIRE_STRATEGY=${RALPH_ENTIRE_STRATEGY:-manual-commit}
      - RALPH_ENTIRE_PUSH_SESSIONS=${RALPH_ENTIRE_PUSH_SESSIONS:-true}
      - RALPH_ENTIRE_LOG_LEVEL=${RALPH_ENTIRE_LOG_LEVEL:-warn}
    # Allow access to host network for Ollama
    # Linux users: set DOCKER_HOST_IP=172.17.0.1 (or your docker0 bridge IP)
    extra_hosts:
      - "host.docker.internal:${DOCKER_HOST_IP:-host-gateway}"
    stdin_open: true
    tty: true
    restart: on-failure

  # LiteLLM proxy - translates Anthropic API to Ollama format
  litellm:
    build:
      context: .
      dockerfile: litellm.Dockerfile
    environment:
      - OLLAMA_API_BASE=http://host.docker.internal:11434
      - LITELLM_DROP_PARAMS=true
      - ANTHROPIC_DISABLE_THINKING=true
    # Linux users: set DOCKER_HOST_IP=172.17.0.1 (or your docker0 bridge IP)
    extra_hosts:
      - "host.docker.internal:${DOCKER_HOST_IP:-host-gateway}"
    ports:
      - "4000:4000"
    volumes:
      - ./litellm-config.yaml:/app/config.yaml
    command: ["--config", "/app/config.yaml", "--port", "4000"]
    healthcheck:
      test: ["CMD", "curl", "-f", "-H", "Authorization: Bearer sk-ralph-local", "http://localhost:4000/health"]
      interval: 10s
      timeout: 5s
      retries: 5
      start_period: 30s
    profiles:
      - ollama

  # Ollama mode (local models) - uses LiteLLM proxy to translate API calls
  ralph-ollama:
    extends: ralph
    depends_on:
      litellm:
        condition: service_healthy
    environment:
      # Point to LiteLLM proxy (Anthropic-compatible endpoint)
      - ANTHROPIC_API_KEY=sk-ralph-local
      - ANTHROPIC_BASE_URL=http://litellm:4000
      # Model name must match litellm config
      - RALPH_MODEL=${RALPH_MODEL:-ollama/qwen2.5-coder:7b}
    profiles:
      - ollama
    restart: on-failure
